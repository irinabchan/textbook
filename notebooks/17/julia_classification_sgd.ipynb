{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "using CSV\n",
    "using DataFrames\n",
    "using MLJ\n",
    "using Statistics\n",
    "using StatsPlots\n",
    "Base.displaysize() = (5, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Logistic Model\n",
    "\n",
    "Previously, we covered batch gradient descent, an algorithm that iteratively updates $\\boldsymbol{\\theta}$ to find the loss-minimizing parameters $\\boldsymbol{\\hat\\theta}$. We also discussed stochastic gradient descent and mini-batch gradient descent, methods that take advantage of statistical theory and parallelized hardware to decrease the time spent training the gradient descent algorithm. In this section, we will apply these concepts to logistic regression and walk through examples using scikit-learn functions.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "The general update formula for batch gradient descent is given by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\alpha \\cdot \\nabla_\\boldsymbol{\\theta} L(\\boldsymbol{\\theta}^{(t)}, \\textbf{X}, \\textbf{y})\n",
    "$$\n",
    "\n",
    "In logistic regression, we use the cross entropy loss as our loss function:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(-y_i \\ln \\left(f_{\\boldsymbol{\\theta}} \\left(\\textbf{X}_i \\right) \\right) - \\left(1 - y_i \\right) \\ln \\left(1 - f_{\\boldsymbol{\\theta}} \\left(\\textbf{X}_i \\right) \\right) \\right)\n",
    "$$\n",
    "\n",
    "The gradient of the cross entropy loss is $\\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y}) = -\\frac{1}{n}\\sum_{i=1}^n(y_i - \\sigma_i)\\textbf{X}_i $. Plugging this into the update formula allows us to find the gradient descent algorithm specific to logistic regression. Letting $ \\sigma_i = f_\\boldsymbol{\\theta}(\\textbf{X}_i) = \\sigma(\\textbf{X}_i \\cdot \\boldsymbol{\\theta}) $:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\theta}^{(t+1)} &= \\boldsymbol{\\theta}^{(t)} - \\alpha \\cdot \\left(- \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) \\textbf{X}_i \\right) \\\\\n",
    "&= \\boldsymbol{\\theta}^{(t)} + \\alpha \\cdot \\left(\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) \\textbf{X}_i \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol{\\theta}^{(t)}$ is the current estimate of $\\boldsymbol{\\theta}$ at iteration $t$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $-\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) \\textbf{X}_i$ is the gradient of the cross entropy loss\n",
    "- $\\boldsymbol{\\theta}^{(t+1)}$ is the next estimate of $\\boldsymbol{\\theta}$ computed by subtracting the product of $\\alpha$ and the cross entropy loss computed at $\\boldsymbol{\\theta}^{(t)}$\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent approximates the gradient of the loss function across all observations using the gradient of the loss of a single data point.The general update formula is below, where $\\ell(\\boldsymbol{\\theta}, \\textbf{X}_i, y_i)$ is the loss function for a single data point:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\alpha \\nabla_\\boldsymbol{\\theta} \\ell(\\boldsymbol{\\theta}, \\textbf{X}_i, y_i)\n",
    "$$\n",
    "\n",
    "Returning back to our example in logistic regression, we approximate the gradient of the cross entropy loss across all data points using the gradient of the cross entropy loss of one data point. This is shown below, with $ \\sigma_i = f_{\\boldsymbol{\\theta}}(\\textbf{X}_i) = \\sigma(\\textbf{X}_i \\cdot \\boldsymbol{\\theta}) $.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\boldsymbol{\\theta} L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y}) &\\approx \\nabla_\\boldsymbol{\\theta} \\ell(\\boldsymbol{\\theta}, \\textbf{X}_i, y_i)\\\\\n",
    "&= -(y_i - \\sigma_i)\\textbf{X}_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When we plug this approximation into the general formula for stochastic gradient descent, we find the stochastic gradient descent update formula for logistic regression.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\theta}^{(t+1)} &= \\boldsymbol{\\theta}^{(t)} - \\alpha \\nabla_\\boldsymbol{\\theta} \\ell(\\boldsymbol{\\theta}, \\textbf{X}_i, y_i) \\\\\n",
    "&= \\boldsymbol{\\theta}^{(t)} + \\alpha \\cdot (y_i - \\sigma_i)\\textbf{X}_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "Similarly, we can approximate the gradient of the cross entropy loss for all observations using a random sample of data points, known as a mini-batch.\n",
    "\n",
    "$$\n",
    "\\nabla_\\boldsymbol{\\theta} L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y}) \\approx \\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}\\nabla_{\\boldsymbol{\\theta}} \\ell(\\boldsymbol{\\theta}, \\textbf{X}_i, y_i)\n",
    "$$\n",
    "\n",
    "We substitute this approximation for the gradient of the cross entropy loss, yielding a mini-batch gradient descent update formula specific to logistic regression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\theta}^{(t+1)} &= \\boldsymbol{\\theta}^{(t)} - \\alpha \\cdot -\\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}(y_i - \\sigma_i)\\textbf{X}_i \\\\\n",
    "&= \\boldsymbol{\\theta}^{(t)} + \\alpha \\cdot \\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}(y_i - \\sigma_i)\\textbf{X}_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in MLJ\n",
    "\n",
    "MLJ provides an interface to the model `SGDClassifier` from the package `ScikitLearn` (you can see the original python's module [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)). Since there is not an available model that implements batch gradient descent, we will compare `SGDClassifier`'s performance against `LogisticClassifier` on the `emails` dataset. We omit feature extraction for brevity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "function dataframe_sample(df, frac)\n",
    "    number_samples = round(Int, frac*nrows(df))\n",
    "    return DataFrame([sample(df[:, col], number_samples) for col in names(df)], names(df))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83-element Array{Int64,1}:\n",
       " ⋮"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "using StatsBase\n",
    "\n",
    "emails = dataframe_sample(CSV.read(\"emails_sgd.csv\"), 0.01)\n",
    "X = emails.email\n",
    "y = emails.spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83×8112 Array{Int64,2}:\n",
       " ⋮      ⋱  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "using TextAnalysis\n",
    "\n",
    "function create_text_matrix(document_array)\n",
    "    crps = Corpus(StringDocument.(document_array))\n",
    "    prepare!(crps, strip_punctuation | strip_case | strip_html_tags | strip_whitespace)\n",
    "    update_lexicon!(crps)\n",
    "    m = DocumentTermMatrix(crps)\n",
    "    return dtm(m, :dense)\n",
    "end\n",
    "\n",
    "X_dense_matrix = create_text_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83-element CategoricalArray{Int64,1,UInt8}:\n",
       " ⋮"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prepared = DataFrame(X_dense_matrix)[:, 1:3000]\n",
    "y_prepared = coerce(y, Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th><th>x9</th><th>x10</th><th>x11</th><th>x12</th><th>x13</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>83 rows × 1,000 columns (omitted printing of 987 columns)</p><tr><th>1</th><td>6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>4</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>5</th><td>18</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & x11 & x12 & x13 & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t5 & 18 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "83×1000 DataFrame\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss = \"log\",\n",
       "              penalty = \"l2\",\n",
       "              alpha = 0.0001,\n",
       "              l1_ratio = 0.15,\n",
       "              fit_intercept = true,\n",
       "              max_iter = 1000,\n",
       "              tol = 0.0001,\n",
       "              shuffle = true,\n",
       "              verbose = 0,\n",
       "              epsilon = 0.1,\n",
       "              n_jobs = nothing,\n",
       "              random_state = 42,\n",
       "              learning_rate = \"optimal\",\n",
       "              eta0 = 0.0,\n",
       "              power_t = 0.5,\n",
       "              early_stopping = false,\n",
       "              validation_fraction = 0.1,\n",
       "              n_iter_no_change = 5,\n",
       "              class_weight = nothing,\n",
       "              warm_start = false,\n",
       "              average = false,)\u001b[34m @ 1…77\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load LogisticClassifier pkg=ScikitLearn\n",
    "@load SGDClassifier pkg=ScikitLearn\n",
    "\n",
    "log_cl = LogisticClassifier(tol=0.0001, random_state=42)\n",
    "sgd_cl = SGDClassifier(tol=0.0001, loss=\"log\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 62\n",
      "    Test set size: 21\n"
     ]
    }
   ],
   "source": [
    "train, test = partition(eachindex(y_prepared), 0.75, shuffle=true)\n",
    "println(\"Training set size: \", length(train))\n",
    "println(\"    Test set size: \", length(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The scitype of `X`, in `machine(model, X, y)` or `machine(model, X, y, w)` is incompatible with `model`:\n",
      "│ scitype(X) = ScientificTypes.Table{AbstractArray{Count,1}}\n",
      "│ input_scitype(model) = ScientificTypes.Table{#s13} where #s13<:(AbstractArray{#s12,1} where #s12<:Continuous). \n",
      "└ @ MLJBase /Users/irinabchan/.julia/packages/MLJBase/sMgCp/src/machines.jl:54\n",
      "┌ Info: Training \u001b[34mMachine{LogisticClassifier} @ 1…19\u001b[39m.\n",
      "└ @ MLJBase /Users/irinabchan/.julia/packages/MLJBase/sMgCp/src/machines.jl:179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.375945 seconds (16.99 k allocations: 4.096 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{LogisticClassifier} @ 1…19\u001b[39m\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mac = machine(log_cl, X_prepared, y_prepared)\n",
    "@time fit!(log_mac, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Stochastic gradient descent is a method that data scientists use to cut down on computational cost and runtime. We can see the value of stochastic gradient descent in logistic regression, since we would only have to calculate the gradient of the cross entropy loss for one observation at each iteration instead of for every observation in batch gradient descent. From the example using scikit-learn's `SGDClassifier`, we observe that stochastic gradient descent may achieve slightly worse evaluation metrics, but drastically improves runtime. On larger datasets or for more complex models, the difference in runtime might be much larger and thus more valuable."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
